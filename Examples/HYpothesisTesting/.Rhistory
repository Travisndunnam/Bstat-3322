round(mean(adjusted),1 ))
```
We now compute the ranndomization distribution of the adjusted values.  We will use 5,000 replications because this is an inportant isssue.
```{r Q1.3}
###########################################################
# We must load the sdimpleboot package for bootstrap      #
# functionality.                                          #
###########################################################
require( simpleboot )
#
#
###########################################################
# Because of the importance of the decsion we use choose  #
# to use 5,000 repliocations.                             #
###########################################################
#
reps <- 5000
#
#
##########################################################
# We compute 5,000 bootstrap samples of the data that    #
# was adjusted to make the null hypothesis true for the  #
# adjusted data.                                         #
##########################################################
#
bootOut <- one.boot( adjusted,
mean,
reps )
#
#
##########################################################
# We extract the bootstrap sampling distribution from    #
# bootOut list.                                          #
##########################################################
#
bsd <- bootOut$t     # Bootstrap sampling distribution
#
#'
##########################################################
# We compute the usual bootstrap diagnostics:            #
#                                                        #
# 1. Bootstrap sampling mean which should be             #
#   approximately equal to the NH value.                 #
#                                                        #
# 2. We also plot the bootstrap sampling distribution    #
#    along with the plotted sample statistic to give us  #
#    a visual interpretation of the pVlaue.              #
##########################################################
#
bsMean <- round( mean( bsd ) )     # Bootstrap sampling
# mean
```
# Computing the p-value
Because we have a lowre tail hypothesis we compute the proportion of observations whose value is less that the $$\bar{y} and divide this number by the number of replications $reps$.  We do this below:
```{r,Q1.4}
lowerTail <- adjusted[ bsd < ybar ]
pValue <- round( length( lowerTail ) / reps, 4 )
xx <- round( 10000 * pValue )
```
The p-value for the null hypotheis is `r pValue`.
## Statistical conclusion
This is evidence that the null hypothesis is incorrect.
*At the 5% or 0.05 level of significance we would reject the null hypothesis.
*There is insufficient evidence to reject the null hypothesis as the 0.01 or 1% level of significance.
## Managerial conclusion
There is some evidence that the pollution level has been reduced.  At the scientific standad of the 5% level of signifcant there is evidence that the pollution level has been decreased.
# Coporate health care
The Human Resources Department of a large corporation wanted to determine if a majority of its employees were satisfied with their treatment by the corporation's health care provider. A random sample of 400 employees was selected, and 275 indicated that they were satisfied with their treatment. Does this data show at the 2% level of significance that a majority of all employees is satisfied? Suppose that the Corporation's president required that more than 60% of employees should be satisfied. Does this data support that requirement at the 2% level of significance?
## The null and alternative hypothesis
We want to show that the proprtion of employees that are staisfied with the corporate heath care provider is greater than 60% which is a proportion of of 0,60.  Hence our null and alternative hypotheis are
\newpage
$$NH: p=0.60$$
$$AH: p>0.60$$
## Synthetic sample
We are not given the orginal sample data but sumerized data.  We know that
263  employees are satisified with the provider and that `r 500-263` are not satisfied.  We use these figures to construct our symethetic sample which I will call $survey$:
```{r Q2.1}
rm( list=ls() )
n <- 500
s <- 323
f <- n - s
survey <- c( rep( 1, s ),
rep( 0, f ))
table(survey)
phat <- round( sum(survey)/ n, 3 )
```
## Computing the point estimate $\hat{p}$
```{r}
phat <- round( s / n )
```
The point estimate for the proprtion is computed as follows
$$\hat{p}=\frac{x}{n}=`r phat`$$
## Adjust  the sample data to make the null hypothesis true
We assume that the null hypothesis is true. For proportions we can do this
thoereticall because under the null hypothesis 60% of the sample sizes should be true and 40% should be false.  Therefore we have
```{r Q2.2}
trueNH <- c( rep( 1, 300 ),
rep( 0, 200 ))
table(trueNH)
```
## Compute the randomization distribution
Because we are using random sample data we can create the bootstrap sampling dostrobution the true NH synethetic sample.
```{r Q2.3}
set.seed( 111 )          # For repriducuability.
require( simpleboot )    # Load simpleboot package.
reps <- 10000
bootOut <-
one.boot( trueNH,
mean,
reps )
bsd <- bootOut$t
ggplot( data.frame( bsd ),
aes(x=bsd)) +
geom_histogram(  binwidth=0.005,
color='blue',
fill='yellow') +
geom_segment( x=phat, y=0,
xend = phat, yend=1100) +
xlab( expression( hat(p) )) +
ylab( "Frequency ") +
ggtitle( "Bootstrap sampling distrubtion of Null True Hypothesis" )
```
## Computing the p-value
The alternative hypothesis is an upper-tail hypothesis.  Therefore we count all the randomization distribution values and divide the replicaltions.
```{r Q2.4}
count <- length( bsd[bsd>phat] )
pValue <- round( count / reps, 4 )
```
$$pValue=`r pValue`$$
## Statistical conclusion
Because the p-value=`r pValue` is is less than 0.02000 there is siffucent evidence to conclude that the null hypothesis is unlikely and we reject it.
## Managerial conclusion
There is sufficent evidence to conclude that the percentage of employees who are satsified with the healthcare plan exceeds 60%.
# 3: Commute Atlanta: Is there a difference mean commutetime driving home between genders
We want dtermine if there is a difference between the mean commute times in Atlanta for different genders.
## The data
The data can be accessed in the R-package $LOck5Data$ in the Coomute Atlanta.  We need to divide the data into two subsramples based on the sex driver.  We are only interest in the variable $Time$ which is the $4^{th}$ variable in the dataset.
## The hypothesis set
We define the following symbols
Parameter | Interpretation
----------|---------------
$\mu_M$   | Male mean commute time
$\mu_F$   Female mean commute time
We can formulate out hypotheses as follows:
$$NH: \mu_M = \mu_F$$
$$AH: \mu_M \ne \mu_F$$
A more useful but equivalent formulation of our hypothesis set is
$$NH: \mu_M - \mu_F = 0$$
$$AH: \mu_M - \mu_F = 0$$
## The data set
The dataset can be found in the R-package $Lock5Data.  The data frame name is $CommuteAtlanta$.  We need to create two subsamples based on gender.  We are only interested in the compute time variable $Time$.
The chunk below shows how to the carry out these tasks.
```{r, Q3.0}
rm(list=ls())
set.seed( 14925 )
require( Lock5Data )
data( CommuteAtlanta )
head( CommuteAtlanta )
Males   <- CommuteAtlanta[CommuteAtlanta$Sex=="M",4]
Females <- CommuteAtlanta[CommuteAtlanta$Sex=="F",4]
```
## Graphical and describptive statistics
We should always polt the data when it is qunatitative.  Because we are comparing nultiple sub-populations we use comparitive boxplots.  Note that we need the orginal data frame $CommuteAtlanta$ to plot this data.
```{r Q3.1}
require( ggplot2 )
ggplot( CommuteAtlanta,
aes( x=Sex, y=Time)) +
geom_boxplot() +
xlab( "Gender" ) +
ylab( "Time in minutes" ) +
ggtitle( "Commute Atlanta \n Commute time by gender")
barM     <- round( mean( Males), 1 )
barF     <- round( mean( Females ), 1 )
barDif  <- barM - barF
```
We have the following sample statistics:
$$\bar{y}_M = `r barM`$$
$$\bar{y}_F =`r barF`$$
$$\bar{y}-\bar{y}_F = `r barDif`$$
## Adjusting the sample data to make the null hypothesis true
We the null hypothis value is zero to adjust the sample to make the null hypotheis true all we need to do is to subtatct the means of each sub-sample:
```{r Q3.2}
adjM <- Males - barM
adjF <- Females - barF
round( mean( adjM ), 1 )
round( mean( adjF ), 1 )
```
## Creating the randomized distribution
Because we have two populations we must use the R-function $two.boot$ in the  $simpleboot$ package.
```{r Q3.3}
require( simpleboot )
bootOut <- two.boot( adjM,
adjF,
mean,
5000 )
rd <- bootOut$t
ggplot( data.frame( rd ),
aes( x = rd )) +
geom_histogram( binwidth=0.5,
color="blue",
fill="yellow") +
xlab( expression(bar(y)[M]-bar(y)[F]) ) +
ylab( "Frequency" ) +
ggtitle( expression( paste("Bootstrap sanpling distribution of ",
bar(y)[M]-bar(y)[F]) ))
```
## Compute the pvalue using the sample statistic
The computation of the p-value is more complex in two tail tests than in single tail tests.  We must include areas from both tails.  If the sampling distribution is syetric we can approximate the p-vlaue by multiply in the p-value as if we had a one-tail by two in the tail that test statstic is found.
```{r !3.4}
count <- length( rd[rd>=barDif])
pValue <- round( 2 * count/5000, 4 )
```
$$p-value=`r pValue`$$
## Statistical conclusion
The p-value of `r pValue` is less than 0.05 so we shoulds reject the sample data is inconsistent with the null hypothesis.
## Magaerial Conclusion
There is mildly strong evidence that the null hypotheis that the mean commute time is the same for both males and females is incorrect.
# Two sample proprotions
The HuffPost Pollster (http://elections.huffingtonpost.com/pollster/2016-iowa-presidential-republican-primary) gives the following numbers for the Iowa Presidential Caucus.
```{r}
n <-401                  #Sample size
nCarson <- 84            # Favor Carson
nTrump  <- 59            # Favor Trump
#
pCarson  <-  nCarson / n
pTrump <-  nTrump / n
pDif <- pCarson - pTrump
#
```
Candidate     | Count | Proprtion
--------------|-------|------------
Carson        | `r nCarson` | `r round( pCarson, 3 )`
Trump         | `r nTrump`  | `r round( pTrump, 3 )`
Is there sufficent evidence at the 5% level of significance that Carson will finish ahead of trump provided the caucus follows the opinion polls.
## The Hypothesis set
Let $p_C$ be the proportion of votes who favor Carson and $p_T$ be the proportion of voters who favor Trump.  Our hypthesus set is as follows:
$$NH: p_C - p_T = 0$$
$$AH: p_C - P_T > 0 $$
## The point estimate
The point estimate of the difference in proportion is
$$ \hat{p}_C -  \hat{p}_T =`r pDif`$$
## Creating the boot sampling distribution
Because out ressults are from a random sample and not an experiment we can use the bootstrap sampling distribution.  We will use 10,000 replications and the $simplleboot$$ package function $$two.boot$ to create the bootstrap distribution.
As before we will need to center the bootstrap distribution so that difference in propotion is equal to zero.  One way to do this is two create sampling at the mean proprtion for both sanples.  We compute this sampleing distribution below.
```{r}
Carson <- c ( rep( 1, nCarson ), rep( 0, n - nCarson ) )
adjCarson <- Carson - pCarson
Trump    <- c(  rep( 1, nTrump),   rep(0, n-nTrump))
adjTrump <- Trump - pTrump
reps <- 10000
require( simpleboot )
bootOut <- two.boot( adjCarson,
adjTrump,
mean,
reps )
bsd <- bootOut$t
```
We compute the mean of the bootstrap sampling distibution.  This value should be approximately equal to zero.  We also should plot the sampling distribution which we hope will v=be symetric and mound shaped.
```{r}
bsdMean <- round( mean( bsd ), 3 )
require( ggplot2 )
ggplot( data.frame( bsd ),
aes(x=bsd )) +
geom_histogram( binwidth=0.01,
color="blue",
fill="yellow") +
xlab( expression( hat(p)[C] - hat(p)[T]) ) +
ylab( "Frequency" ) +
ggtitle( expression(
paste( "Reference distribution of NH:",
p[C] - p[T]==0)))
```
The mean of the reference distribiution is equal to 0 to 3 decimal places. The reference distribution  looks faily symetric and mound shaped.  Therefore we can proceed with computing the p-value.
## Computing the p-value
The alternative hypothesis specifies upper tail test.  We therefore we compute the p-value:
```{r}
pValue <- round( ( length( bsd[ bsd >= pDif] )/ reps), 4)
```
The p-values is `r pValue`.
## Statistical conclusion
Because the p-value=`r pValue` is less than 0.0500.  Thhere is relatively strong evidence to teject the null hypothesis that Carson and Trump are tied.
## Subject matter conclusion
There is relatively strong evidence that Mr. Carson will win the Iowa Caucus if the Caucus is held on the dame day as the survey.
# Cocaine addition
In the statistical theory of design of experiments, randomization involves randomly allocating the experimental units across the treatment groups. For example, if an experiment compares a new drug against a standard drug, then the patients should be allocated to either the new drug or to the standard drug control using randomization.
We are going to analyze two analyze the following helthcare problem:
> In a randomized experiment on treating cocaine addiction, 48 people were randomly assigned to take either Desipramine (a new drug), or Lithium (an existing drug), and then followed to see who relapsed
The research question of interest: Is Desipramine better than Lithium at treating cocaine addiction?
We will use the following notation for the experiment:
$pD=$the proportion of individuals receicing Desipramine who did not relapse
$pL=$the proprtion of individuals receiving lithium who did not relapse.
The sample data is in the R-package $Lock5Data$.  The data frame name is named $CocainTreatment$.  The data is given below.
```{r}
rm( list=ls())
require( Lock5Data )
data("CocaineTreatment")
CT <- CocaineTreatment
head(CT)
tail(CT)
```
We need to compute the proportion of individuals who relapsed.  We will do this as follows:
```{r}
outcome <- table( CT )
outcome
pD <- outcome[1,1] / 24
pL <- outcome[2,1] / 24
pDif <- pD - pL
```
## Hypothesis set
Our research hypothesis of interest is that using Desipramine will have a lower relapse proportion than patients who receive Lithium.  Our hypothesis set is:
$$NH: p_D=pL$$
$$NH: pD=pL$$
Equivalently we can rewrite the above hypothesis set as
$$NH: p_D-pL=0$$
$$NH: pD-pL <0$$
## Creating the ranomization distribution
Because in experiments treatments are randomized to experimental subjects our reference distribution should be created in the same way.  We can do this by using the R-function $sample.int$$.  WE will show how to do this below for a single ranomization sample.
```{r}
#
###########################################################
# We need to change the strings to numeric o's and 1's so #
# we can use the mean function to compute proportions.    #
###########################################################
#
y <- ifelse(CT$Relapse[1:48]=="yes", 0, 1 )
#
#
###########################################################
# We create an index to assign treatments to the random   #
# Groups D and L.                                         #
###########################################################
#
All <- 1:48
#
#
###########################################################
# We ranomly assign 24 of the values in y to group D.     #
###########################################################
#
D <- sample.int( 48, 24, replace=FALSE)
D
y[D]
#
#
###########################################################
# We assign any obkects not assiged to group D to group   #
# group L.                                                #
###########################################################
L <- All[-A]
L[B]
#
#
##########################################################
# We compute the difference between the completely       #
# randomized groups.                                     #
##########################################################
#
pDif <- mean(y[D]) - mean( y[L])
```
In practive we need to do this a large number of times.  The code below will create randomization distribution with 20,000 replcations using the drug data.
```{r}
reps <- 20000
rd   <- rep( NA, reps)
outcome <- ifelse(CT$Relapse[1:48]=="yes", 0, 1 )
trials   <- 1:48
for( i in 1:reps){
D <- sample.int( 48, 24, replace=FALSE )
L <- trials[-D]
rd[i]= mean( y[D] - y[L])
}
```
The mean of the randomization should be approximately equal to zero.  Its value is `r round( mean( rd), 3)`.  This value is close to zero.
We plot the randomization distribution below.
```{r}
require( ggplot2 )
ggplot(  data.frame(rd),
aes(x=rd)) +
geom_histogram( binwidth=0.10,
color="blue",
fill="yellow")
```
ggplot(  data.frame(rd),
aes(x=rd)) +
geom_histogram( binwidth=0.05,
color="blue",
fill="yellow")
#
###########################################################
# We need to change the strings to numeric o's and 1's so #
# we can use the mean function to compute proportions.    #
###########################################################
#
y <- ifelse(CT$Relapse[1:48]=="yes", 0, 1 )
#
#
###########################################################
# We create an index to assign treatments to the random   #
# Groups D and L.                                         #
###########################################################
#
All <- 1:48
#
#
###########################################################
# We ranomly assign 24 of the values in y to group D.     #
###########################################################
#
D <- sample.int( 48, 24, replace=FALSE)
D
y[D]
#
#
###########################################################
# We assign any obkects not assiged to group D to group   #
# group L.                                                #
###########################################################
L <- All[-D]
L
y[L]
#
#
##########################################################
# We compute the difference between the completely       #
# randomized groups.                                     #
##########################################################
#
pDif <- mean(y[D]) - mean( y[L])
pValue <- length( rd[rd <= pDif] )
pValue <- round( length( rd[rd <= pDif] ) / reps, 4 )
rd[ rd <- pDif]
rd
require( ggplot2 )
ggplot(  data.frame(rd),
aes(x=rd)) +
geom_histogram( binwidth=0.10,
color="blue",
fill="yellow")
outcome <- table( CT )
outcome
pD <- outcome[1,1] / 24
pL <- outcome[2,1] / 24
pDif <- pD - pL
#
###########################################################
# We need to change the strings to numeric o's and 1's so #
# we can use the mean function to compute proportions.    #
###########################################################
#
y <- ifelse(CT$Relapse[1:48]=="yes", 0, 1 )
#
#
###########################################################
# We create an index to assign treatments to the random   #
# Groups D and L.                                         #
###########################################################
#
All <- 1:48
#
#
###########################################################
# We ranomly assign 24 of the values in y to group D.     #
###########################################################
#
D <- sample.int( 48, 24, replace=FALSE)
D
y[D]
#
#
###########################################################
# We assign any obkects not assiged to group D to group   #
# group L.                                                #
###########################################################
L <- All[-D]
L
y[L]
#
#
##########################################################
# We compute the difference between the completely       #
# randomized groups.                                     #
##########################################################
#
pDif <- mean(y[D]) - mean( y[L])
reps <- 20000
rd   <- rep( NA, reps)
outcome <- ifelse(CT$Relapse[1:48]=="yes", 0, 1 )
trials   <- 1:48
for( i in 1:reps){
D <- sample.int( 48, 24, replace=FALSE )
L <- trials[-D]
rd[i]= mean( y[D] - y[L])
}
require( ggplot2 )
ggplot(  data.frame(rd),
aes(x=rd)) +
geom_histogram( binwidth=0.10,
color="blue",
fill="yellow")
pValue <- round( length( rd[rd <= pDif] ) / reps, 4 )
ECHO <- TRUE
rm( list=ls())
rm( list=ls())
require( Lock5Data )
data("CocaineTreatment")
CT <- CocaineTreatment
head(CT)
tail(CT)
outcome <- table( CT )
outcome
pD <- outcome[1,1] / 24
pL <- outcome[2,1] / 24
pDif <- pD - pL
