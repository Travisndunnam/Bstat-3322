{
    "contents" : "---\ntitle: \"Simple Linear Regression\"\nauthor: \"by Craig W. Slinkman\"\ndate: \"December 1, 2015\"\noutput: \n  html_document: \n    number_sections: yes\n    toc: yes\n---\n# Simple Linear Regression\nSimple linear regression is a technique in parametric statistics that is commonly used for analyzing mean response of a variable Y which changes according to the magnitude of an predictor variable X.\n\n# The theoretical model\nThe simple linear regression is given by\n\n$$y_i=\\beta_0 + \\beta_i x_i + e_i~~~~~~~~~i=1,n$$\nWhere\n\n$~~~~~~~~~~~~~~~~y_i$ is the $i^{th}$ response variable  \n\n$~~~~~~~~~~~~~~~~x_i$ is the $i^{th}$ predictor variable  \n\n$~~~~~~~~~~~~~~~~\\beta_0$ is the regression intercept coefficient  parameter   \n\n$~~~~~~~~~~~~~~~~\\beta_1$ is the regression slope coefficient  \n\n$~~~~~~~~~~~~~~~~e_i$ is the unobservable random error term\n\n# Simple linear assumptions\n* We assume the statistical relationship is a linear function.  \n\n* The mean of the distribution of error terms is equal to zero.  Symbolically we have  \n\n$$E(e_i=0)$$\n\n* The variance of error terms is constant.  Symbolically we have \n$$Var(Y|X=x_i)=\\sigma^2$$\n\n* The error terms are statistically independent.  If the error terms are normally distributed then this implies that their correlation should be zero.  \n\n* If we are going to use the model for prediction than the  of the error terms should be normally distributed.  This last assumption can be written \n\n$$e_i \\sim N(0,\\sigma^2)$$\n\nNote that this assumption is include the constant variance assumption as well as the normality assumption.  A normal distribution must have constant variance.  \n\n\n\n\n## Examples\nWe resent three examples that we use through out these notes.  The background story for each of the data sets will follow.  \n\n1. The first example will be Singfat Chu's example of the pricing of \"starter\" diamond rings. \n\n2. The second example is the tipping behavior at a bistro.  \n\n3.  The third example is attempting to predict students grades on the final exam as a function of their grade on the first exam.\n\n## Singapore diamond ring data\nThe source of the data is a full page advertisement placed in the Straits Times newspaper issue of February 29, 1992, by a Singapore-based retailer of diamond jewelry.  \n\nThe advertisement contained pictures of diamond rings and listed their prices, diamond content, and gold purity. Only 20K ladies' rings, each mounted with a single diamond stone, were considered for this study. 20K rings are made with gold of 20 carat purity. (Pure gold is rated as 24K.)  \n\nThere were 48 such rings of varying designs. The weights of the diamond stones ranged from 0.12 to 0.35 carats (a one carat diamond stone weighs 0.2 gram) and were priced between $223 and $1086. The jewelry store adopted a fixed-price policy.  \n\n## The pricing of diamond rings\nIn Singapore, the pricing of gold jewelry is simple. The price equals the current market value of the gold content (i.e., weight times the going rate per gram of gold) plus a craftsmanship fee.  \n\nHowever, the pricing of other jewelry like diamond rings is more complicated because they are not as standardized as gold jewelry. The price of diamond jewelry depends on the four C's: caratage, cut, color, and clarity of the diamond stone. A good cut gives a diamond more sparkle. Colorless diamonds are the most prized. A flawless diamond has maximum clarity because the passage of light is unimpeded through the stone. Cut, color, and clarity are subjective factors and are very hard for the layman to gauge.\n\n## Obtaining the data\nWe can read the data set directly from the web location using the following R-script.  \n\n```{r getRings}\nRINGS <- \"http://www.amstat.org/publications/jse/datasets/diamond.dat.txt\"\nRings <- \n    read.table( RINGS, header=FALSE )\ncolnames( Rings ) <- c( \"Carats\", \"Price\" )\n\nhead( Rings )\ntail( Rings )\n```\n\nIn this example we are predicting the _Price_.  Therefore, the response variable is the weight of the diamond.  The weight of the diamond is _Carats_.  A scatterplot of _Price_ versus _Carats_ is shown below.  It appears that over the _Carats_ the _Price_ appears to change in a linear manner.  \n\n```{r scatterRings}\n\nrequire( ggplot2 )\n\nggplot( Rings,\n        aes( x=Carats, y=Price)) +\n    geom_jitter( ) +\n    geom_smooth( method=lm, \n                 se=FALSE ) +\n    geom_smooth( method=loess,\n                 se=FALSE,\n                 color=\"red\") +\n    annotate( \"text\", \n              x=0.15, y=900,\n              label=\"OLS Line\",\n              hjust=0,\n              size=3,\n              color=\"blue\") +\n    annotate( \"text\", \n              x=0.15, y=825,\n              label=\"LOESS Line\",\n              hjust=0,\n              size=3,\n              color=\"Red\") +\n    xlab( \"Carats\" ) +\n    ylab( \"Price (Singapore Dollars)\" ) +\n    ggtitle( \"Singapore Starter Diamond Rings\"  ) +\n    theme_bw()\n\n```\n\n  \n It appears that the relationship is linear because the blue OLS line and the red LOESS line are almost coincident.  Further, we can see that this is a very strong relationship because the data points are very close to the fitted OLS line.\n\n### Determing the best fitting line\n\n\n### Fitting the linerar model\nWe fit OLS models using the R-function $lm$.  We use this function to estimate the intercept and slope estimates of the simple linear regression.  We demonstrate this below.\n\n```{r fitPrice}\nslrRings <- lm( Price ~ Carats,\n                data=Rings)\nsummary( slrRings )\n\n```\n\nThe estimated regression model is\n$$\\hat{E}(Price|X=Carats) = -259.63 + 372102.02 Carats  $$\n\nGiven that the _Price_ variable is relatively large compared to decimal parts of the regression we can rewrite the above equation as\n\n$$\\hat{E}(Price|X=Carats) = -260 + 3721Carats$$  \n\nIf we want to stay with our $x$ and $y$ notation we can write the above equation as\n\n$$\\hat{y}=-260+3721x$$\n\nNote that $\\hat{E}(Price|X=Carats)$ and $\\hat{y}$ are called the fitted values by some statisticians.  Others call these values the predicted values.  I tend to use both of these terms.  \n\n### Interpretation of the estimated regression coefficients\nYou always must be very careful on interpreting the meaning of the regression coefficients.  This is particularly true of the intercept coefficient $\\hat{\\beta}_0$.\n\n#### Interpretation of $\\hat{\\beta}_0$\nLook at the scatterplot of _Price_ versus _Carats_.  Notice that there are no values of _Carat_ that have a value of zero or are negative.  That is the intercept value is outside the range of the predictor variable _Carats_.  When this occurs __we cannot give an economic interpretation of the inrtercept coefficient__. \n\nHence for this dataset no interpretation is possible for this data set.  \n\n### The interpretation of the slope coefficient $\\hat{\\beta}_1$ \n\nThe estimated slope coefficient $\\hat{\\beta}_1$ is the estimated  change in the response variable per unit change  in the predictor variable.  Thus our interpretation of the estimated slope coefficient is that we estimate on average that the value of a diamond ring increases by $3721 per _Carat_ increase.  \n\nNote that the range of data is much less than one carat.  We easily see this from the scatterplot.  It is more reasonable and easier to understand if we take an 0.01 carat increment.  Our interpretation then becomes that we estimated the mean value of the diamond ring changes by approximately $36 per 0.01 carat increase. \n\n### Fitted values and residuals\n$\\hat{y}$ is the symbol that represents the predicted values for the OLS line of best fit in linear regression. The equation for the fitted values is\n $$\\hat{y}=\\hat{\\beta}_0 + \\hat{\\beta}_1 x$$  \n \n To obtain the fitted values with R we use the R-function fitted.  We obtain the fitted values below.  \n \nThe residuals are difference between the response variable and the fitted value for all observations in the data set.  Writing this as an equation we have\n$$\\hat{e}=y_i - \\hat{y}_i~~~~~i=1,...,n$$\n\nNote the $^$ over the $e$.  What does this mean.  Recall that the basic linear regression model is \n$$y_i=\\beta_0 + \\beta_1x_i+e_i$$\n\nThe $e_i$ are the unobservable error terms.  The residuals are the estimated error terms for your data set.  Since they are estimated we use $^$ to signify that we are estimating them. We compute the residuals as follows:\n\n$$\\hat{e}_i=\\hat{y}_i - y_i$$\n\n### Properties of residuals\n1. The sum of the residuals is always equal to zero.\n\n2. This implies that the mean of the residuals in the sample will be equal to zero\n\n3. In turn this implies that the sum of the predicted values is equal to the sum of the response variables. \n\n4. There should no pattern in the residuals. We can't use the value of one variable to predict the value of another variable.  \n\n\n### Computing and adding the fittied values and residuals to the regression data set\n\nUsing R compute the fitted values $\\hat{y}$ and the residuals $\\hat{e}$ as follows:\n\n```{r fittedPrice}\n\nRings$yHat <- fitted( slrRings )\nRings$eHat <- residuals( slrRings )\nhead( Rings )\n```\n\n### Partioning the variation of Y into variation predicted by the model and variation not predicted by the model\nInspect the the plot of fitted model given below.\n\n```{r partitionRing}\nset.seed( 1123 )\nn <- 25\nX <- round( rnorm( n, 80, 5 ))\nY <- 40 + 0.75 * X\nY <- round(Y + rnorm( length(Y), 0, 6))\nX<- round( X )\ny <- round( Y )\nXbar <- mean( X )\nYbar <- mean( Y )\nMeans <- data.frame( Xbar, Ybar )\nDF <- data.frame( X, Y )\nfit <- lm( Y ~ X, data=DF)\nDF$Yhat <- fitted.values( fit )\nhead(DF)\n\nggplot( DF,\n        aes(x=X, y=Y )) +\n    geom_point() +\n    geom_smooth( method=lm, \n                 se=FALSE ) +\n    geom_point( aes( y=Yhat),\n                color=\"blue\") +\n    geom_point( data=Means,\n                aes( x=Xbar, y=Ybar),\n                size=4,\n                color=\"blue\") +\n    geom_vline( x=Xbar,\n                linetype=2 ) +\n    geom_hline( y=Ybar,\n                linetype=2 ) +\n    annotate( \"text\",\n                x=Xbar+0.5, y=91,\n                label=\"bar(X)\",\n                parse=TRUE,\n                hjust=0 ) +\n     annotate( \"text\",\n                x=73, y=Ybar+2,\n                label=\"bar(Y)\",\n                parse=TRUE,\n                hjust=0 ) +\n    annotate( \"segment\",\n              x=86, y=105.43654,\n              xend=86, yend=114,\n              color=\"red\",\n              linetype=2 ) +\n    annotate( \"segment\",\n               x=86, y=Ybar,\n               xend=86, yend=105.44,\n               linetype=2,\n               color=\"blue\") +\n    annotate( \"text\", \n               x=86.25, y=111,\n               label=\"Residual\",\n               size=3,\n               hjust=0,\n               size=3,\n               color=\"red\" ) +\n    annotate( \"text\", \n               x=86.25, y=103.5,\n               label=\"Predicted variation\",\n               size=3,\n               hjust=0,\n               color=\"blue\" ) \n```            \n             \nLooking at the plot we that for the $i^{}th$ point that the following relationship holds\n\n$$(y_i-\\bar{y})=(\\hat{y}_i-\\bar{y})+(y_i-\\hat{y}_i)$$\n\nThat is, the at data point i the the deviation of the response variable $y_i-\\bar{y})$ is equal to the sum of the deviation of the response from the mean of the response variable plus the sum of the residual $(y_i-\\hat{y}_i)$.  \n\nIf we square both sides of above equation and sum over all $n$ observations a miracle occurs and we obtain the following equation\n\n$$\\Sigma_{i=1}^n (y_i- \\bar{y})^2=\\Sigma_{i=1}^n ( \\hat{y}_i - \\bar{y} )^2 + \\Sigma_{i=1}^n ( y_i - \\hat{y}_i )^2 $$\n\nWe call the above equation the decomposition or partitioning of the sums of squares.\n\nNow the term of the left is numerator of the variance of $y$.  It is a measure of how variability of $y$.  We call this quantity the sum-of-squares of $y$ and denote it as\n$SYY$\n\n$SYY=\\Sigma_{i=1}^n (y_i- \\bar{y})^2$$\n\nThe last term of the right side of decomposition of the sums of squares is the sum-of-squares of the residuals.  We call this quantity the residual sum of squares\n\n$$SSR = \\Sigma_{i=1}^n ( y_i - \\hat{y}_i )^2=\\Sigma_{i=1}^n \\hat{e}_i^2$$\n\nBecause the residuals are the prediction error we want this quantity to be ass small possible.\n\nThe last quantity $\\Sigma_{i=1}^n ( \\hat{y}_i - \\bar{y} )^2$ is the reduction in prediction errors when we use the predictor to predict the response.  We will denote this by $SS_{Reg}$ or the sum of squares prediction.  So we have \n\n$$SS_{Reg} = $\\Sigma_{i=1}^n ( \\hat{y}_i - \\bar{y} )^2$$\n\nWe can rewrite the decomposition of the sums-of-squares as\n\n$$SYY=SS_{Reg}+RSS$$\n\n### Obtaining the decompostion of the sums-of squares\nA standard analytical technique is called the analysis of variance.  We can obtain the decomposition of the sums-of-squares by asking R to give it to us.  We do this below.  \n\n```{r ringsAnova}\nanova( fit )\n```\n\nThe sums of squares decomposition for our hypothetical is\n\nSource     | SS \n-----------|--------  \n$SS_{Reg}$ |  209.43  \n$RSS$      |  909.53  \n$SYY$      |  `r  209.43 +  909.53`  \n\nR does not give $SYY$ so we had to compute it by hand.  \n\n### The degrees of freedom\n the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom.  \n \nIn this class it is equal to the sample size minus the number of parameters we estimate before we can compute the residual sums of squares.  The table below gives the number of degrees of freedom for the one population mean model and the simple linear population model.  \n\n$$E[Y|X=x_i]=\\beta_0~~~~~~~~~~~~~~~~~~df=n-1$$\n$$E[Y|X=x_i]=\\beta_0 + \\beta_1 x_i~~~~~~df=n-2$$\n\nNote we only count linear parameters we do not count  standard deviations.\n\n### Mean square residuals\nThe mean square residuals is a variance.  Mean squares are always equal to a sums of squares divided by the degrees of freedom for the specific sums of squares.  For simple linear regression this is equal to \n\n$$S^2=MSR=\\frac{RSS}{n-2}$$  \nThe $MSR$ is the estimated variance of the unobservable error terms.  \n\nTo obtain this value in R we use the ANOVA table.\n\n```{r, echo=FALSE}\nanova( fit )\n```\n\n### The estimated standard devation of the residuals\nThe estimated variance of the unobservable residuals is given by\n\n$$s_y.x^2 = \\frac{RSS}{n-2}$$\n  \n```{r,echo=FALSE}\nanova( fit )\n```\n\n\n### The coefficient of determination $R^2$\nRecall that is $SS_{Reg}$ measure the relative predictive ability of our regression.  To be able to compare models we would like a standardized measure.  The standardized measure is call the coefficient of determination and is denoted by $R^2$.  We define as follows  \n\n$$R^2 = \\frac{SS_{Reg}}{SYY}=1-\\frac{RSS}{SYY}$$\n\n$$R^2 is a number that indicates how well data fit a statistical model - sometimes simply a line or a curve. An $R^2$ of 1 indicates that the regression line perfectly fits the data, while an $R^2$ of 0 indicates that the line does not fit the data at all. This latter can be because the data is utterly non-linear, or because it is random.\n\n$R^2$ is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, as the proportion of total variation of outcomes explained by the model.\n\n### Obtaining the $R^2$\nWe obtain the coefficient of variation by using R's summary function for linear models.  The R-code below obtains the coefficient of determination for our Rings data.\n\n```{r ringsR2}\n\nsummary( slrRings )\n\n```\n\nOn R's output the $R^2$ is called the multiple $R^2$.  This is an old that is no longer in use.  The value is given\n \n$$R^2= 0.9783$$  \n\nWe interpret this value in the our diamond ring context that approximately 98% of the variation in the diamond ring prices is associated with the variation in the variation of the weight of the ring  in Carats.  \n\n### Estimating the standard deviaiton of the un-obsevable error terms  \nTo estimate the estimated standard deviation of the error terms we first estimate the mean square residuals.  We do this by dividing the residual sum-of-squared by the degrees of freedom.  \n\nTechnically, the degrees of freedom are difficult to understand.  The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom. In other words, the number of degrees of freedom can be defined as the minimum number of independent coordinates that can specify the position of the system completely.\n\nHowever the is a fairly simple rule that will allow us to compute the degrees of freedom when we are using linear statistical models.  We simply subtract from the sample size $n$ the number of parameters we are estimating.  \n\nIn simple linear regression we are estimating $\\beta_0$ and $\\beta_1$.  Thus the degrees of freedom is \n$$df=n-2$$  \n\nThus the mean-square-residual is given by\n\n$$MSR=\\frac{RSS}{n-2}$$\n\nThe mean square residual is found the ANOVA table with the label _Residuals_.  The $ANOVA$ table is reprinted below.  \n\n```{r ringsANOVA2}\nanova( slrRings)\n```\n\nTherefor we have:\n\n$$MSR=1014$$\n\n### The estimated standard deviation of the error terms\nR call the estimated standard deviation of the residuals _Residual standard error_.  This is found on the model summary output. \n\n```{r}\nsummary( slrRings )\n```\n\nIn this class we will denote the estimated standard deviation of the residuals by $s_{y.x}$.  This is computed by $s_{y.x}$  \n\n$$s_{y.x}=\\sqrt{MSR}=\\sqrt{\\frac{RSS}{n-2}}$$\n\nFrom the R-output we find \n$$s_{y.x}=31.84 \\approx 32$$\n\n### Interpretation of $s_{y.x}$\nThe estimated standard deviation of the error terms is a standard deviation.  Provided the errors are normally distributed we can use the empirical rule to estimate the proportion of response variable values that fall within 1, 2, or 3 standard deviations.  The table below demonstrates this.  \n\nPercentage | Bounds  \n-----------|----------\n68%        | $\\pm 32$  \n95%        | $\\pm 64$  \n99.7%      | $\\pm 96$  \n\nThus we expect approximately 95% of the observations to be with $64 of the mean regression line.  Another way to say this is that 95% of the random error terms will be less than $64.  \n\nThis is an approximation.  We can check this by counting the the number of residuals with an absolute less than $64.  \n\n### Checking model validity with graphs\nWe call the process of checking a model to see if the data satisfy the assumption of linearity, constant variance, and normality _diagnostic checking_.  \n\nRaju Rimal ( https://rpubs.com/therimalaya/43190) has written an R-function called $diagPlot$ to perform the graphical analysis for us.  The function can be found in the working directory of the SLR.Rmd file.  To use this function we must use a source statement to define the function so we can use it. We demonstrate this function for the diamond ring price data set below.\n\n```{r}\nsource( \"diagPLot.R\")\ndiagPlot( slrRings )\n```\n\n#### Assessing linearity\nIf the relationship between the response variables and the predictor variables is linear then a scatterplot of the residuals $\\hat{e}_i$ versus the predicted values $\\hat{e}_i$ should be a null plot.  \n\nIn practice we draw this plot and add a loess smooth.  If the lowess smooth is an approximate horizontal line then there is no evidence on non-linearity.  \n\nRecall that the end points of the loess have the greatest variability and unless there a consistent systematic departure of the loess than we conclude that the linearity assumption is correct.  \n\nFor the diamond ring price data inspection of the residual versus versus fitted values plot is consistent with a linear model.  \n\n#### Assessing normality\nTo assess the normality of the error terms we draw a normal QQ plot of the residuals $\\hat{e}_i$.  \n\nThe inspection of the normal QQ plot for the diamond ring price data shows little evidence that the data is not normal.  \n\n#### Constant data assumption\nWe check the constant variance assumption by plotting the square root of the absolute value of the residuals $\\sqrt{|\\hat{e}_i|}$ versus the predicted values $\\hat{Y}_i$.  We then use the lowess smoothing technique to smooth the plot.  This loess smooth should be approximately horizontal line. This smooth is much more variable than the fitted $\\hat{e}_i$ versus predicted \\hat{y}_i$ line.  \n\nI=inspecting the line line for the diamond rings data we find that the line is highly variable but there is some indication that the variance increases after $600 dollars.  \nWe couple this with the normal QQ plot of the residuals and conclude that over the range of the data the variance appears to be relatively constant.\n\n#### Influential observations\nThe diagnostic plot function adds two more plots that assess the impact of points analyzed one at a time on the estimated regression coefficients.  \n\nWe will not address these plots here because they are an advanced topic other than to comment that as long as as the Cook's distance is less than 1.00 there are know data observations that are usually no problems with your data,  \n\n### Statistical Inference\nIn this section we address the following statistical inference.  The same general rules apply to linear regression analysis that apply to univariate statistical inference. In this section we assume that the error terms are normally distributed.  \n\nWe will address the following statistical inferences:\n\n* Hypothesis test for model utility\n* Confidence intervals for the regression coefficients  \n* Hypothesis tests for the regression coefficients  \n* Confidence interval for estimated regression line\n\n#### Hypothesis test for model utility\nSuppose we fit a simple linear regression model to a data set.  Our standard model is  \n\n$$E(Y|X=x_i)=\\beta_0+\\beta_1x_i$$\n\nIf we assume  that a linear relationship does not exist between the response variable and the predictor variable then $\\beta_1`=0$ and we have\n \n$$E(Y|X=x_i)=\\beta_0+0 \\times x_i=\\beta_0$$  \n\nIt is a statistical fact that if we use the OLS estimation procedure  for the model \n\n$$E(Y|X=x_i)=\\beta_0$$ \n\nWe find that $\\hat{\\beta_0}=\\bar{y}$\n\nIn other words if the response variable is not linearly related to the predictor variable then the OLS estimator of the intercept coefficient is the sample mean. \n\nHence we have two possible models.  The first model is the mean model $E(Y|X=x_i)=\\beta_0$.  The more more complex model is the simple linear regression model $E(Y|X=x_i)=\\beta_0+\\beta_1x_1$.  By complexity we mean a model with more estimated parameters. The question we want to ask is \"What do we gain by adding an addition estimated parameter to the simple model?\"\n\nWe formalize this for hypothesis testing.  Our null hypothesis is that there is no linear relationship between the predictor variable and the response variable.  That is \n\n$$NH:~E(Y|X)=\\beta_0$$\n\nThe alternative hypothesis is that there is a linear relationship between the predictor variable and the response variable.  \n\nThe plot below shows the null hypothesis model and the alternative hypothesis model for the diam ring price data. \n```{r}\n\nCarats <- rep( Rings$Carats, 2 )\npriceAH <- fitted( slrRings )\nm <- length( priceAH )\npriceNH <- rep( mean( Rings$Price  ), m)\nPrice   <- c( priceNH, priceAH )\nHypothesis <- c( rep( 'NH', m),\n                 rep( 'AH', m))\nDF <- data.frame(  Carats, \n                   Price, \n                   Hypothesis  )\n\nggplot( DF,\n        aes( x=Carats, \n             y=Price, \n             linetype=Hypothesis )) +\n    geom_smooth( method=lm, color=\"black\" ) +\n    scale_linetype_discrete(name  =\"Hypothesis\",\n                     breaks=c(\"NH\", \"AH\")) +\n    annotate( \"text\",\n              x=0.10, y=900,\n              label=\"NH: E(Y|X)==beta[0]\",\n              parse=TRUE,\n              hjust=0,\n              size=4,\n              color=\"black\") + \n    annotate( \"text\",\n              x=0.10, y=825,\n              label=\"AH: E(Y | X)==beta[0]+beta[1]*x\",\n              parse=TRUE,\n              hjust=0,\n              size=4,\n              color=\"black\") +\n    ggtitle( \"Null and Alternative Models \") +\n    theme_bw()\n\n```\n\nA logically equivalent set of hypotheses is\n\n$$NH:~\\beta_1=0$$\n$$AH:~\\beta_1 \\ne 0$$\n\nThe test statistic for this test is computed by the ratio of the mean squares.  These can be found on the ANOVA table.  \n\nFor the diamond ring price data we have \n```{r}\nanova(slrRings )\n```\n\nThe mean square regression is found on the _Carats_ row.  The mean residual sum of squares if found on the _Residuals_ row.  The ratio of these two quantities gives an F-test statistic.  We will not use this test statistic in this class but will instead rely on the p-value.  We see the p-value is very small\n\n$$pValue=\\frac{2.2}{10,000,000,000,000,000} \\approx 0$$  \n\nTherefore we have very strong sample evidence that the null hypothesis is incorrect.  \n\nNote that the $F$ test statistic can be found on the R's regression summary output.  It is found on the last land.  This saves you from having to run the $anova$ function.  \n\n```{r}\nsummary( slrRings )\n```\n\nBe aware that the test for model utility is a two tail test.  It simply tests the following hypothesis set\n\n$NH:~No~linear~relation~between~the~predictor~and~response~ variables$ \n$AH:~There~is~a~linear~relationship~between~the~predictor~and~the~response variables$  \n\nYou should also be aware that if you have a monotonic nonlinear relationship this test will probably reject the null hypothesis.  It is not a test for linearity but a test to tell if predictor variable is useful in predicting the response variable.  \n\n__There is no excuse for the failure to plot and look!__\n\n#### Confidence intervals for the regression coefficients\nWe know turn to computing the confidence intervals for the regression coefficients.  We will not discuss the formulas used to compute these but will rely on R to do this for us. \n##### The intercept coefficient\nUsually the intercept coefficient has not interpretation and only serves to minimize the residual sum of squares.  When this situation arises it makes no sense to compute the confidence interval for the intercept.  \n\nHowever, occasionally we do need to compute a confidence interval for the coefficient because it has meaning in the context of the managerial situation.  \n\nTo compute the confidence interval for the intercept we use the R-function $confint$.  \n\nSuppose we want to compute the 98% confidence interval for \nthe diamond ring prices data.  We do as follows:\n\n```{r}\nconfint( slrRings, 1, 0.98 )\n```\n\nIt makes no sense to interpret this confidence interval because there is not an observation has a values less than or equal to zero in the diamond ring price data set.  \n\n#### The confidence intercept for the slope\nWe use the same R-function to compute the confidence interval.  A 98% confidence interval for the slope coefficient is found by\n\n```{r}\nconfint( slrRings, 2, 0.98 )\n```\n\nWe can also compute confidence intervals for all the regression coefficients by \n\n```{r}\nconfint( slrRings, level=0.98 )\n```\n\n#### Plotting the slope coefficient\nWe can also plot the confidence interval coefficients (See http://www.r-bloggers.com/coefplot-new-package-for-plotting-model-coefficients/).  \n\nTo do so we must install and require the package _coefplot_.  We plot the coefficients below:\n\n```{r}\nrequire( coefplot )\ncoefplot( slrRings )\n```\n\nWe have already seen the the intercept has no interpretation and therefore we can remove it from our plot by creating a coefficient data.frame.  \n\n```{r}\ncoefplot( slrRings,\n          intercept=FALSE )\n```\n\n#### Interpretation of coefficients and confidence intervals \nWe previously discussed that the slope coefficient for the diamond rings data set is difficult to interpret because it is a per carat increase in a data set that does not even have 0.5 carat in it.  \n\nWe had decided that we will interpret the regression coefficients for 0.01 carat increase.  Thus our estimate is computed as follows:\n\n```{r}\nslope <- round( coef( slrRings )[2] * 0.01, 2 )\nslope \n```\nLikewise we get the 98% confidence lower and upper confidence bounds:\n\n```{r}\ninterval <- \n    round( confint( slrRings, \n                    level=0.98 )[2,] /\n                     100, 2 )\n\n```\nOur managerial interpretation of slope coefficient for the diamond rings price data set is as follows:\n\n>We estimate the the average marginal change  in the price of a diamond ring is approximately $37.20 per 0.01 additional carat.  We are 98% sure that the true but unknown average marginal increase in the value is between $35.20 and $39.30.  \n\nWe can also give the confidence interval as $35.20 \\pm 2$.  \n\n#### Hypothesis test of the regression coefficients.  \nUnless you plan to do theoretical research in business or economics you will not be likely to need to perform hypotheses test on the regression coefficients.  Therefore we will not cover them at this time.  \n\n#### Confidence interval for the estimated regression line\nThe confidence interval of the regression line is drawn for your automatically by the plotting2 function geom_smooth.  This region is shown below.  \n\n```{r}\nbp <- \n    ggplot( Rings ,\n        aes( x=Carats, y=Price)) +\n    geom_point() +\n    geom_smooth( method=lm )\n```\n\nThe shaded grey area is the confidence for the mean value of y for each value of x in the range of the data set, that is, $\\hat{E}[Y|X=x] \\pm ME[\\hat{E}[Y|X=x]$.\n\nThere are times when we are not interested in the prediction of the response variable at a specific value of the predictor but want to know the average value of the response variable at the value of the predictor variable.  Because we are working with averages we are computing confidence intervals for the average response variable given a value of the predictor.  \n\nThe equations to predict a new response variable value $\\hat{Y}_{Mean}$ at value of $X_{Mean}$ are \n\n$$\\hat{E}Y_{Mean}=\\hat{\\beta}_0+\\hat{\\beta}_1 x_{Mean}$$  \n\n$$SXX=\\Sigma_{i=1}^2(x_i-\\bar{x})^2$$\n\n$$~~~~~~~~~~~~~~~~~~\\widehat{se}_{Mean} = s_{y.x} \\sqrt{ \\frac{1}{n} + \\frac{(x_{Mean} - \\hat{x})}{SXX}}$$\n\n$$~~~~~~~~~~~\\widehat{me}_{Mean}=\\sqrt{F(a-\\alpha/2;1,n-2)}\\widehat{se}_{Mean}$$\n\nThe prediction interval is computed like a conference interval \n\n $$\\hat{Y}_{Mean} \\pm \\widehat{se}_{Mean}$$\n\nAs you can see it is a fairly complicated process to estimate the average value of y at a specific value of x.  You would need to learn how to look up the quantiles of the F distribution which has two different degrees of freedom. \n\nLuckily for us R can can do this for us.  Suppose we want to estimate the average price of 0.25 carat ring. We first need to create a variable with _exactly_ the same name as the predictor variable is __slrRings__.  \n\n```{r}\nCarats <- 0.25\n```\n\nWe now create a new data frame.  For this example I will call it __Predictor__.  \n\n```{r}\n( Predictor <- data.frame( Carats ))\n```\n\n\n-----\n##### Description\n\n###### Predicted values based on linear model object\n\n__Usage__\n\n__S3 method for class 'lm'__  \n\n```\npredict(object, \n         newdata, \n         se.fit = FALSE, \n         df = Inf,\n         interval = c(\"none\", \"confidence\", \"prediction\"),\n        level = 0.95 )\n        \n```\n\nWe apply this to a $slrRings$ data to compute a 98% confidence interval for the mean diamond ring price when the diamond weight is 0.25 carats.\n\n```{r}\nEstimation <-\n    predict( slrRings,\n             Predictor,\n             df=48-2,\n             interval=\"confidence\",\n             level=0.98 )\nEstimation <- \n    round( Estimation )\nEstimation\n\n```\n\nWe estimate that the average price of a 0.25 carat ring is $\\$`r Estimation [1,1]`$ We are 98% sure that the true but unknown true mean price is between $\\$`r Estimation[1,2]`$  and $\\$`r Estimation[1,3]`$.\n\n### Predicting new values\nWe first start by illustrating the difference between estimation of a mean and predicting a new value.  \n\nConsider the average high temperature for December 8 according to AccuWeather.com ( http://www.accuweather.com/en/us/arlington-tx/76010/December-weather/331134 ) is $58^{\\circ} F$.  This is an estimate of the  average of the temperature  December 8 for all December 8^{ts}. \n\nIn contrast the predicted temperature for Arlington Texas on December 8, 2015 is $70^{\\circ}F$.  This is a prediction for a single day.\n\nThe equations to predict a new response variable value $\\hat{Y}_{Pred}$ at value of $X_{Pred}$ are \n\n$$\\hat{Y}_{Pred}=\\hat{\\beta}_0+\\hat{\\beta}_1 x_{Pred}$$  \n\n$$SXX=\\Sigma_{i=1}^2(x_i-\\bar{x})^2$$\n\n$$~~~~~~~~~~~~~~~~~~\\widehat{se}_{Pred} = s_{y.x} \\sqrt{1 + \\frac{1}{n} + \\frac{(x_{Pred} - \\hat{x})}{SXX}}$$\n\n$$~~~~~~~~~~~\\widehat{me}_{Pred}=\\sqrt{F(a-\\alpha/2;1,n-2)} \\widehat{se}_{Pred}$$\n\nThe prediction interval is computed like a conference interval \n\n $$\\hat{Y}_{Pred} \\pm \\widehat{se}_{Pred}$$\n\nSuppose we want to predict the price of a diamond ring that weighs 0.25 carats.  We use the same approach as before but change the __interval__ parameter to \"pred\".  \n\n```{r}\n\n( Predictor <- data.frame( Carats=0.25 ))\n\nPrediction <-\n   predict( slrRings,\n             Predictor,\n             df=48-2,\n             interval=\"pred\",\n             level=0.98 )\nPrediction <-\n    round( Prediction )\nPrediction\n\n```\n\nThe predicted price of a 0.25 carat diamond ring is $\\$`r Prediction[1,1]`$. The probability is 98% that the price of the ring will be between $\\$`r Prediction[1,2]`$ and $\\$`r Prediction[1,3]`$.  \n\n#### Facts about prediction\nThe following facts appear about using linear regression for prediction.  \n* The predicted value is always an estimated expected value.  Thus, the estimated mean and the predicted value are equal.  \n\n* The standard error of the prediction is always wider than the standard error of estimation.  \n\n* The prediction intervals are always wider than the estimation intervals.  \n\n* Unlike confidence intervals prediction intervals are probability statements.  \n\n#### Plotting simple linear regression prediction intervals\nSuppose we want to produce a graph that shows the predictions over the entire range of possible predictor values.  The code below shows how easy this is to do using ggplot2.  \n\n```{r}\nxmin <- min( Rings$Carats)\nxmax <- max( Rings$Carats )\nxmin  <- round( xmin, 1 )\nxmax  <- round( xmax, 1 )\nCarats <- seq( xmin, xmax, 0.01 )\nPredictor <- data.frame( Carats )\nPrediction <-\n   predict( slrRings,\n             Predictor,\n             df=48-2,\n             interval=\"pred\",\n             level=0.98 )\nPrediction <-\n    round( Prediction )\nPrediction <- \n    data.frame( Predictor, Prediction )\nhead( Prediction )\ntail( Prediction )\n\nggplot( Prediction,\n        aes( x=Carats, y=fit )) +\n    geom_line() +\n    geom_ribbon(aes( x=Carats,\n                     ymin=lwr, \n                     ymax=upr, \n                     fill = \"band\",\n                     alpha = 0.3)) +\n    geom_point( data=Rings, aes(x=Carats, y=Price)) +\n    ylab( \"Price ($)\" ) +\n    ggtitle( \"Diamond Ring Price Predictions and 98% Prediction Interval\" ) +\n    theme( legend.position=\"none\")\n    \n```\n\n#### Extraploation and Interpolation\nWhen we are estimating or predicting a new value the confidence we have in our result depends upon where the predictor variable is within the data set.  If predictor is greater than the minimum predictor variable value and less than or equal than the maximum value in the data set then we are interpolation.  \n\nInterpolation is safe because we have information on the behavior of the data.\n\nIf we are outside the range of the data then we are extrapolation.  We have no information about the behavior of the data.  The model may not be linear or the variability of the residuals may increase. \n\nThus it is risky to use the model outside of the range of data used to fit it. ",
    "created" : 1449617308172.000,
    "dirty" : false,
    "encoding" : "ISO8859-1",
    "folds" : "",
    "hash" : "2858534703",
    "id" : "5E485776",
    "lastKnownWriteTime" : 1453258179,
    "path" : "C:/Users/Craig/BSTAT3322/SLR/SimpleLinearRegression.Rmd",
    "project_path" : "SimpleLinearRegression.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}