---
title: "Simple Linear Regression"
author: "by Craig W. Slinkman"
date: "December 1, 2015"
output: html_document
---
# Simple Linear Regression
Simple linear regression is a technique in parametric statistics that is commonly used for analyzing mean response of a variable Y which changes according to the magnitude of an predictor variable X.

## Examples
We resent three examples that we use through out these notes.  The background story for each of the data sets will follow.  

1. The first example will be Singfat Chu's example of the pricing of "starter" diamond rings. 

2. The second example is the tipping behavior at a bistro.  

3.  The third example is attempting to predict students grades on the final exam as a function of their grade on the first exam.

## Singapore diamond ring data
The source of the data is a full page advertisement placed in the Straits Times newspaper issue of February 29, 1992, by a Singapore-based retailer of diamond jewelry.  

The advertisement contained pictures of diamond rings and listed their prices, diamond content, and gold purity. Only 20K ladies' rings, each mounted with a single diamond stone, were considered for this study. 20K rings are made with gold of 20 carat purity. (Pure gold is rated as 24K.)  

There were 48 such rings of varying designs. The weights of the diamond stones ranged from 0.12 to 0.35 carats (a one carat diamond stone weighs 0.2 gram) and were priced between $223 and $1086. The jewelry store adopted a fixed-price policy.  

## The pricing of diamond rings
In Singapore, the pricing of gold jewelry is simple. The price equals the current market value of the gold content (i.e., weight times the going rate per gram of gold) plus a craftsmanship fee.  

However, the pricing of other jewelry like diamond rings is more complicated because they are not as standardized as gold jewelry. The price of diamond jewelry depends on the four C's: caratage, cut, color, and clarity of the diamond stone. A good cut gives a diamond more sparkle. Colorless diamonds are the most prized. A flawless diamond has maximum clarity because the passage of light is unimpeded through the stone. Cut, color, and clarity are subjective factors and are very hard for the layman to gauge.

## Obtaining the data
We can read the data set directly from the web location using the following R-script.  

```{r getRings}
RINGS <- "http://www.amstat.org/publications/jse/datasets/diamond.dat.txt"
Rings <- 
    read.table( RINGS, header=FALSE )
colnames( Rings ) <- c( "Carats", "Price" )

head( Rings )
tail( Rings )
```

In this example we are predicting the _Price_.  Therefore, the response variable is the weight of the diamond.  The weight of the diamond is _Carats_.  A scatterplot of _Price_ versus _Carats_ is shown below.  It appears that over the _Carats_ the _Price_ appears to change in a linear manner.  

```{r scatterRings}

require( ggplot2 )

ggplot( Rings,
        aes( x=Carats, y=Price)) +
    geom_jitter( ) +
    geom_smooth( method=lm, 
                 se=FALSE ) +
    geom_smooth( method=loess,
                 se=FALSE,
                 color="red") +
    annotate( "text", 
              x=0.15, y=900,
              label="OLS Line",
              hjust=0,
              size=3,
              color="blue") +
    annotate( "text", 
              x=0.15, y=825,
              label="LOESS Line",
              hjust=0,
              size=3,
              color="Red") +
    xlab( "Carats" ) +
    ylab( "Price (Singapore Dollars)" ) +
    ggtitle( "Singapore Starter Diamond Rings"  ) +
    theme_bw()

```

  
 It appears that the relationship is linear because the blue OLS line and the red LOESS line are almost coincident.  Further, we can see that this is a very strong relationship because the data points are very close to the fitted OLS line.
 
### Fitting the linerar model
We fit OLS models using the R-function $lm$.  We use this function to estimate the intercept and slope estimates of the simple linear regression.  We demonstrate this below.

```{r fitPrice}
slrRings <- lm( Price ~ Carats,
                data=Rings)
summary( slrRings )

```

The estimated regression model is
$$\hat{E}(Price|X=Carats) = -259.63 + 372102.02 Carats  $$

Given that the _Price_ variable is relatively large compared to decimal parts of the regression we can rewrite the above equation as

$$\hat{E}(Price|X=Carats) = -260 + 3721Carats$$  

If we want to stay with our $x$ and $y$ notation we can write the above equation as

$$\hat{y}=-260+3721x$$

Note that $\hat{E}(Price|X=Carats)$ and $\hat{y}$ are called the fitted values by some statisticians.  Others call these values the predicted values.  I tend to use both of these terms.  

### Interpretation of the estimated regression coefficients
You always must be very careful on interpreting the meaning of the regression coefficients.  This is particularly true of the intercept coefficient $\hat{\beta}_0$.

#### Interpretation of $\hat{\beta}_0$
Look at the scatterplot of _Price_ versus _Carats_.  Notice that there are no values of _Carat_ that have a value of zero or are negative.  That is the intercept value is outside the range of the predictor variable _Carats_.  When this occurs __we cannot give an economic interpretation of the inrtercept coefficient__. 

Hence for this dataset no interpretation is possible for this data set.  

### The interpretation of the slope coefficient $\hat{\beta}_1$ 

The estimated slope coefficient $\hat{\beta}_1$ is the estimated  change in the response variable per unit change  in the predictor variable.  Thus our interpretation of the estimated slope coefficient is that we estimate on average that the value of a diamond ring increases by $3721 per _Carat_ increase.  

Note that the range of data is much less than one carat.  We easily see this from the scatterplot.  It is more reasonable and easier to understand if we take an 0.01 carat increment.  Our interpretation then becomes that we estimated the mean value of the diamond ring changes by approximately $36 per 0.01 carat increase. 

### Fitted values and residuals
$\hat{y}$ is the symbol that represents the predicted values for the OLS line of best fit in linear regression. The equation for the fitted values is
 $$\hat{y}=\hat{\beta}_0 + \hat{\beta}_1 x$$  
 
 To obtain the fitted values with R we use the R-function fitted.  We obtain the fitted values below.  
 
The residuals are difference between the response variable and the fitted value for all observations in the data set.  Writing this as an equation we have
$$\hat{e}=y_i - \hat{y}_i~~~~~i=1,...,n$$

Note the $^$ over the $e$.  What does this mean.  Recall that the basic linear regression model is 
$$y_i=\beta_0 + \beta_1x_i+e_i$$

The $e_i$ are the unobservable error terms.  The residuals are the estimated error terms for your data set.  Since they are estimated we use $^$ to signify that we are estimating them. We compute the residuals as follows:

$$\hat{e}_i=\hat{y}_i - y_i$$

### Properties of residuals
1. The sum of the residuals is always equal to zero.

2. This implies that the mean of the residuals in the sample will be equal to zero

3. In turn this implies that the sum of the predicted values is equal to the sum of the response variables.  

### Computing and adding the fittied values and residuals to the regression data set

Using R compute the fitted values $\hat{y}$ and the residuals $\hat{e}$ as follows:

```{r fittedPrice}

Rings$yHat <- fitted( slrRings )
Rings$eHat <- residuals( slrRings )
head( Rings )

```

### Partioning the variation of Y into variation predicted by the model and variation not predicted by the model
Inspect the the plot of fitted model given below.

```{r partitionRing}
set.seed( 1123 )
n <- 25
X <- round( rnorm( n, 80, 5 ))
Y <- 40 + 0.75 * X
Y <- round(Y + rnorm( length(Y), 0, 6))
X<- round( X )
y <- round( Y )
Xbar <- mean( X )
Ybar <- mean( Y )
Means <- data.frame( Xbar, Ybar )
DF <- data.frame( X, Y )
fit <- lm( Y ~ X, data=DF)
DF$Yhat <- fitted.values( fit )
head(DF)

ggplot( DF,
        aes(x=X, y=Y )) +
    geom_point() +
    geom_smooth( method=lm, 
                 se=FALSE ) +
    geom_point( aes( y=Yhat),
                color="blue") +
    geom_point( data=Means,
                aes( x=Xbar, y=Ybar),
                size=4,
                color="blue") +
    geom_vline( x=Xbar,
                linetype=2 ) +
    geom_hline( y=Ybar,
                linetype=2 ) +
    annotate( "text",
                x=Xbar+0.5, y=91,
                label="bar(X)",
                parse=TRUE,
                hjust=0 ) +
     annotate( "text",
                x=73, y=Ybar+2,
                label="bar(Y)",
                parse=TRUE,
                hjust=0 ) +
    annotate( "segment",
              x=86, y=105.43654,
              xend=86, yend=114,
              color="red",
              linetype=2 ) +
    annotate( "segment",
               x=86, y=Ybar,
               xend=86, yend=105.44,
               linetype=2,
               color="blue") +
    annotate( "text", 
               x=86.25, y=111,
               label="Residual",
               size=3,
               hjust=0,
               size=3,
               color="red" ) +
    annotate( "text", 
               x=86.25, y=103.5,
               label="Predicted variation",
               size=3,
               hjust=0,
               color="blue" ) 
```            
             
Looking at the plot we that for the $i^{}th$ point that the following relationship holds

$$(y_i-\bar{y})=(\hat{y}_i-\bar{y})+(y_i-\hat{y}_i)$$

That is, the at data point i the the deviation of the response variable $y_i-\bar{y})$ is equal to the sum of the deviation of the response from the mean of the response variable plus the sum of the residual $(y_i-\hat{y}_i)$.  

If we square both sides of above equation and sum over all $n$ observations a miracle occurs and we obtain the following equation

$$\Sigma_{i=1}^n (y_i- \bar{y})^2=\Sigma_{i=1}^n ( \hat{y}_i - \bar{y} )^2 + \Sigma_{i=1}^n ( y_i - \hat{y}_i )^2 $$

We call the above equation the decomposition or partitioning of the sums of squares.

Now the term of the left is numerator of the variance of $y$.  It is a measure of how variability of $y$.  We call this quantity the sum-of-squares of $y$ and denote it as
$SYY$

$SYY=\Sigma_{i=1}^n (y_i- \bar{y})^2$$

The last term of the right side of decomposition of the sums of squares is the sum-of-squares of the residuals.  We call this quantity the residual sum of squares

$$SSR = \Sigma_{i=1}^n ( y_i - \hat{y}_i )^2=\Sigma_{i=1}^n \hat{e}_i^2$$

Because the residuals are the prediction error we want this quantity to be ass small possible.

The last quantity $\Sigma_{i=1}^n ( \hat{y}_i - \bar{y} )^2$ is the reduction in prediction errors when we use the predictor to predict the response.  We will denote this by $SS_{Reg}$ or the sum of squares prediction.  So we have 

$$SS_{Reg} = $\Sigma_{i=1}^n ( \hat{y}_i - \bar{y} )^2$$

We can rewrite the decomposition of the sums-of-squares as

$$SYY=SS_{Reg}+RSS$$

#### Obtaining the decompostion of the sums-of squares
A standard analytical technique is called the analysis of variance.  We can obtain the decomposition of the sums-of-squares by asking R to give it to us.  We do this below.  

```{r ringsAnova}
anova( fit )
```

The sums of squares decomposition for our hypothetical is

Source     | SS 
-----------|--------  
$SS_{Reg}$ |  209.43  
$RSS$      |  909.53  
$SYY$      |  `r  209.43 +  909.53`  

R does not give $SYY$ so we had to compute it by hand.  

### The coefficient of determination $R^2$
Recall that is $SS_{Reg}$ measure the relative predictive ability of our regression.  To be able to compare models we would like a standardized measure.  The standardized measure is call the coefficient of determination and is denoted by $R^2$.  We define as follows  

$$R^2 = \frac{SS_{Reg}}{SYY}=1-\frac{RSS}{SYY}$$

$$R^2 is a number that indicates how well data fit a statistical model - sometimes simply a line or a curve. An $R^2$ of 1 indicates that the regression line perfectly fits the data, while an $R^2$ of 0 indicates that the line does not fit the data at all. This latter can be because the data is utterly non-linear, or because it is random.

$R^2$ is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, as the proportion of total variation of outcomes explained by the model.

### Obtaining the $R^2$
We obtain the coefficient of variation by using R's summary function for linear models.  The R-code below obtains the coefficient of determination for our Rings data.

```{r ringsR2}

summary( slrRings )

```

On R's output the $R^2$ is called the multiple $R^2$.  This is an old that is no longer in use.  The value is given
 
$$R^2= 0.9783$$  

We interpret this value in the our diamond ring context that approximately 98% of the variation in the diamond ring prices is associated with the variation in the variation of the weight of the ring  in Carats.  

### Estimating the standard deviaiton of the un-obsevable error terms  
To estimate the estimated standard devatwion of the error terms we first estimate the mean square residuals.  We do this by dividing the residual sum-of-squared by the degrees of frredom.  

Technically, the degrees of freedom are difficult to understand.  The number of independent ways by which a dynamic system can move, without violating any constraint imposed on it, is called number of degrees of freedom. In other words, the number of degrees of freedom can be defined as the minimum number of independent coordinates that can specify the position of the system completely.

However the is a fiarly simple rule that will allow us to compute the degrees of frredon when we are using linear statistical models.  We simply subtract from the sample size $n$ the number of parameters we are estimating.  

In simple linear regression we are estimating $\beta_0$ and $\beta_1$.  Thus the degrees of freedom is 
$$df=n-2$$  

Thus the mean-square-residual is given by

$$MSR=\frac{RSS}{n-2}$$

The mean square residual is found the ANOVA table with the label _Residuals_.  The anovat table is reprinted below.  

```{r ringsANOVA2}
anova( slrRings)
```

Therefor we have:

$$MSR=1014$$

### Estimated standard devation of errors

## Restaurant tips

### Description
The owner of a bistro called First Crush in Potsdam, NY was interested in studying the tipping patterns of his customers. He collected restaurant bills over a two week period that he believes provide a good sample of his customers. The data recorded from 157 bills include the amount of the bill, size of the tip, percentage tip, number of customers in the group, whether or not a credit card was used, day of the week, and a coded identity of the server.  

## The data set

A dataset with 157 observations on the following 7 variables.

Bill - Size of the bill (in dollars)  
Tip - Size of the tip (in dollars)
Credit -Paid with a credit card? n or y
Guests - Number of people in the group
Day	 Day of the week: m=Monday, t=Tuesday,  th=Thursday, or f=Friday
Server - Code for waiter/waitress: A, B, or C
PctTip	- Tip as a percentage of the bill
  
Our initial objective is to is to predict the amount of the tip based only on the amount of the bill.  

The data is in the __Lock5Data__ package.  We load the data and data as follows.  

```{r getTips}

require( Lock5Data )
data( RestaurantTips )
head( RestaurantTips )
tail( RestaurantTips )
```

We now draw the scatterplot using _Bill_ as the predictor variable and _Tip_ as the response variable.

```{r scatterTips}

ggplot( RestaurantTips,
        aes( x=Bill, y=Tip )) +
    geom_jitter( ) +
    geom_smooth( method=lm, 
                 se=FALSE ) +
    geom_smooth( method=loess,
                 se=FALSE,
                 color="red") +
    annotate( "text", 
              x=5, y=10,
              label="OLS Line",
              hjust=0,
              size=3,
              color="blue") +
    annotate( "text", 
              x=5, y=9,
              label="LOESS Line",
              hjust=0,
              size=3,
              color="Red") +
    xlab( "Bill ($)" ) +
    ylab( "Tip ($)" ) +
    ggtitle( "Tips at the Bristro"  ) +
    theme_bw() 

```
  
We see from the scatterplot that a linear relationship appears to be appropriate.  In addition, we see that the variance of the data increases as the bill increase so the variance is not constant.  For now though we will pretend that the variance is constant. 

### Fitting the linerar model
We fit OLS models using the R-function $lm$.  We use this function to estimate the intercept and slope estimates of the simple linear regression.  We demonstrate this below.

```{r FtTips}

slrTips <- 
    lm( Tip ~ Bill,
        data=RestaurantTips )
summary( slrTips )
```

Our estimated or fitted model is

$$E(TIP|X=Bill)=`r round( coef(slrTips)[1],2)` + `r round( coef(slrTips)[2],2)`x$$

### Interpretation of the estimated regression coefficients
You always must be very careful on interpreting the meaning of the regression coefficients.  This is particularly true of the intercept coefficient $\hat{\beta}_0$.

#### Interpretation of $\hat{\beta}_0$
We see that there are no meals with r bill of zero. Hence for this dataset no interpretation is possible for this data set.  

### The interpretation of the slope coefficient $\hat{\beta}_1$ 

The estimated slope coefficient $\hat{\beta}_1$ is the estimated  change in the response variable per unit change in the predictor variable.  In this case we estimate that the mean change is the tip given is $0.18 per dollar increase in the bill.  Another way of saying this for this specific data this that on average we estimate that customer tipping rate is 18%.    



## Predicting the score on the final from the score on the first exam  

We are interest to see if we can predict the final exam score from the first exam score.  We will use the data frame in the in the __Lock5Data)__ package names _StatGrades_.  The data set is loaded below.  

```{r getGrades}

data( StatGrades )
head( StatGrades )
tail( StatGrades )

```
  
Clearly the predictor variable is the score on the first exam and the response variable is response variable.  We draw the scatterplot below.  

```{r}

ggplot( StatGrades,
        aes( x=Exam1, y=Final )) +
    geom_jitter( ) +
    geom_smooth( method=lm, 
                 se=FALSE ) +
    geom_smooth( method=loess,
                 se=FALSE,
                 color="red") +
    annotate( "text", 
              x=20, y=90,
              label="OLS Line",
              hjust=0,
              size=3,
              color="blue") +
    annotate( "text", 
              x=20, y=85,
              label="LOESS Line",
              hjust=0,
              size=3,
              color="Red") +
    xlab( "First exam core (100 possible pints)" ) +
    ylab( "Final exam score (100 possible points)" ) +
    ggtitle( "Final exam Score versus first exam score"  )+
    theme_bw()

```

We see that the LOESS are roughly coincident when the score on the first exam is greater than 80.  However, below 80 we we find non-linear behavior.  This may be due to outliers.  We can better analyze this after we fit the data. 

### Fitting the linerar model
We fit OLS models using the R-function $lm$.  We use this function to estimate the intercept and slope estimates of the simple linear regression.  We demonstrate this below.

```{r fitScores}

slrScores <- 
    lm( Final ~ Exam1,
        data=StatGrades )
summary( slrScores )

```
  
The estimated regression equation is

$$\hat{E}(Final|x=Exam1)=`r round(coef(slrScores)[1],2)` +
`r round(coef(slrScores)[2],2)`x$$
  
### Interpretation of $\hat{\beta}_0$
Refer to the scatterplot of the final exam score versus the first exam score.  We find no observations that are close to zero.  Hence there is no practical explanation of the intercept term.  

### Interpretation of $\hat{\beta}_1$
The interpretation of the estimated slope coefficient is that we estimate on average an additional point scored on the first exam will increase the score on the final exam by approximately 0.62 points.
