---
title: "0020-Simple Linear Regression"
author: "by Craig W. Slinkman"
date: "November 24, 2015"
output: html_document
---

The simple linear regression model consists of the mean function and the variance function:

$E(Y|X=x) = \beta_0 + \beta_1 x$  
$Var(Y|X=x) = \sigma^2$

In other words we have  

*The mean value of Y is a linear function of x.  

*The variance of Y is constant.

The mathematical symbols are $\beta_0$, $\beta_1$, and $\signa^2$ are unknown population parameters and must be estimated from the data.

# Graph of a population model
A hypothetical population model is shown below.


```{r}
rm( list=ls())
X <- -2:10
Y <- 5.0 + 0.5 * X
DF <- data.frame( X, Y )

require( ggplot2 )
ggplot( DF,
        aes( x=X, y=Y)) + 
    geom_smooth( method=lm,
                    color="red" ) +
    annotate( "text",
               x=0+0.20, y=5+0.50,
               label="beta[0]",
              parse=TRUE,
              hjust=0,
              vjust=0) +
    annotate( "segment",
               x=-2.5, xend=0,
               y=5, yend=5,
              linetype=3) +
    annotate( "segment",
              x=0, xend=0,
              y=0, yend=10 ) +
    annotate( "segment",
              x=-2, xend=10,
              y=0, yend=0) +
    annotate( "segment",
              x=4, xend=5,
              y = 7, yend=7) +
    annotate( "segment",
              x=5, xend=5,
              y=7, yend=7.5 ) +
    annotate( "text",
              x=4.5, y=7-0.5,
              label="beta[1]",
              parse=TRUE) +
    annotate( "text",
              x=2, y=9,
              label="E(Y|X=x)=5 +0.25x",
              color="red",
              hjust=0 ) +
    ggtitle( "Population regression model" )
```

Because the variance $\sigma^2>0$, the observed value of the ith response $y_i$ will typically not equal its expected value $E(Y|X=x_i)$. To account for this difference between the observed data and the expected value, statisticians have invented a quantity called a statistical error, or $e_i$, for case i defined implicitly by the equation

$$y_i=E(Y|X=x_i) + e_i$$

or explicitly by

$$e_i=y_i-E(Y|X=x_1)$$

To illustrate this point we will draw a random sample of 10 observations from the $Heights$ data and show the fitted (fitted means estimated )regression line and show all the errors.  We are assuming we are dealing with population data here.  In reality we will use estimated errors which are called _residuals_.  

```{r}
rm( list=ls())
require( alr4 )
data(Heights)
fit <- lm( dheight ~ mheight, data=Heights)
betas <- coef( fit )

set.seed( 11235 )
i <- sample.int( dim(Heights)[1], 10, replace=FALSE )
Heights$error  <- residuals(fit)
Heights$fit    <-  fit$fitted.values

Heights <- Heights[i, ]
i <- sample.int( dim(Heights)[1], 10, replace=FALSE )

require( ggplot2 )

p <-
    ggplot( Heights,
        aes(x=mheight, y=dheight))  +
        geom_point() +
    geom_abline( intercept = betas[1], 
                 slope = betas[2],
                 color="red")
for( i in 1:10 )
{
    p <- p +
        annotate( "segment",
                  x=Heights$mheight[i],
                  xend=Heights$mheight[i],
                  y=Heights$fit[i],
                  yend=Heights$dheight[i],
                  color="blue")
}

p + ggtitle( "Random Error Terms" )  
    
```

The errors $e_i$ depend on unknown parameters in the mean function and so are not observable quantities. They are random variables and correspond to the vertical distance between the point $y_i$ and the mean function $E(Y|X=x_i)$. In the Heights data, the errors are the differences between the heights of particular daughters and the average height of all daughters with mothers of a given fixed height.  

# Assumptions about the error terms $e_i$
In order to determine how we can estimate the parameters we must make certain assumptions about the error terms $e_i$.  

We make two important assumptions concerning the errors. First, we assume that $E( E_i|X=x_i)=0$, so if we could draw a scatterplot of the $e_i$ versus the $x_i$, we would have a null scatterplot, with no patterns. 

The second assumption is that the errors are all independent, meaning that the value of the error for one case gives no information about the value of the error for another case. This is likely to be true in the examples in this class , although this assumption will not hold in all problems.

Errors are often assumed to be normally distributed, but normality is much stronger than we need. In this book, the normality assumption is used primarily to obtain tests and confidence statements with small samples where $n \le 30$. If the errors are thought to follow some different distribution, such as the Poisson or the binomial, other methods besides $OLS$ may be more appropriate.





